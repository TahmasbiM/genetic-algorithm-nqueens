{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TahmasbiM/genetic-algorithm-nqueens/blob/master/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "VjIsJLNVrcH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "a53ef859-c228-49f1-dc4b-700849c13f40"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from datetime import datetime\n",
        "import glob, os\n",
        "from tensorboardX import \n",
        "\n",
        "env = gym.make('MountainCar-v0')\n",
        "env.seed(1); torch.manual_seed(1); np.random.seed(1)\n",
        "PATH = glob.glob(os.path.expanduser('~/tboardlogs/'))\n",
        "writer = t('~/tboardlogs/{}'.format(datetime.now().strftime('%b%d_%H-%M-%S')))\n",
        "\n",
        "\n",
        "max_position = -.4\n",
        "positions = np.ndarray([0,2])\n",
        "rewards = []\n",
        "successful = []\n",
        "for episode in range(1000):\n",
        "    running_reward = 0\n",
        "    env.reset()\n",
        "    done = False\n",
        "    for i in range(200):\n",
        "        state, reward, done, _ = env.step(np.random.randint(0,3))\n",
        "        # Give a reward for reaching a new maximum position\n",
        "        if state[0] > max_position:\n",
        "            max_position = state[0]\n",
        "            positions = np.append(positions, [[episode, max_position]], axis=0)\n",
        "            running_reward += 10\n",
        "        else:\n",
        "            running_reward += reward\n",
        "        if done: \n",
        "            if state[0] >= 0.5:\n",
        "                successful.append(episode)\n",
        "            rewards.append(running_reward)\n",
        "            break\n",
        "\n",
        "print('Furthest Position: {}'.format(max_position))\n",
        "plt.figure(1, figsize=[10,5])\n",
        "plt.subplot(211)\n",
        "plt.plot(positions[:,0], positions[:,1])\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Furthest Position')\n",
        "plt.subplot(212)\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.show()\n",
        "print('successful episodes: {}'.format(np.count_nonzero(successful)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-06b1cfdb76fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/tboardlogs/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/tboardlogs/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%b%d_%H-%M-%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SummaryWriter' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "1PKIU268smsb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I tried different weight initializations but found they did not perform well.\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        nn.init.normal_(m.weight, 0, 1)\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.state_space = env.observation_space.shape[0]\n",
        "        self.action_space = env.action_space.n\n",
        "        self.hidden = 200\n",
        "        self.l1 = nn.Linear(self.state_space, self.hidden, bias=False)\n",
        "        self.l2 = nn.Linear(self.hidden, self.action_space, bias=False)\n",
        "    \n",
        "    def forward(self, x):    \n",
        "        model = torch.nn.Sequential(\n",
        "            self.l1,\n",
        "            self.l2,\n",
        "        )\n",
        "        return model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8NCfXx8cspXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "fc0baf60-4e76-4174-f340-a5f5ba633303"
      },
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "steps = 2000\n",
        "state = env.reset()\n",
        "epsilon = 0.3\n",
        "gamma = 0.99\n",
        "loss_history = []\n",
        "reward_history = []\n",
        "episodes = 3000\n",
        "max_position = -0.4\n",
        "learning_rate = 0.001\n",
        "successes = 0\n",
        "position = []\n",
        "\n",
        "# Initialize Policy\n",
        "policy = Policy()\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(policy.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "\n",
        "for episode in trange(episodes):\n",
        "    episode_loss = 0\n",
        "    episode_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for s in range(steps):\n",
        "        # Uncomment to render environment\n",
        "        #if episode % 100 == 0 and episode > 0:\n",
        "        #    env.render()\n",
        "        \n",
        "        # Get first action value function\n",
        "        Q = policy(Variable(torch.from_numpy(state).type(torch.FloatTensor)))\n",
        "        \n",
        "        # Choose epsilon-greedy action\n",
        "        if np.random.rand(1) < epsilon:\n",
        "            action = np.random.randint(0,3)\n",
        "        else:\n",
        "            _, action = torch.max(Q, -1)\n",
        "            action = action.item()\n",
        "        \n",
        "        # Step forward and receive next state and reward\n",
        "        state_1, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # Find max Q for t+1 state\n",
        "        Q1 = policy(Variable(torch.from_numpy(state_1).type(torch.FloatTensor)))\n",
        "        maxQ1, _ = torch.max(Q1, -1)\n",
        "        \n",
        "        # Create target Q value for training the policy\n",
        "        Q_target = Q.clone()\n",
        "        Q_target = Variable(Q_target.data)\n",
        "        Q_target[action] = reward + torch.mul(maxQ1.detach(), gamma)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = loss_fn(Q, Q_target)\n",
        "        \n",
        "        # Update policy\n",
        "        policy.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Record history\n",
        "        episode_loss += loss.item()\n",
        "        episode_reward += reward\n",
        "        # Keep track of max position\n",
        "        if state_1[0] > max_position:\n",
        "            max_position = state_1[0]\n",
        "            writer.add_scalar('data/max_position', max_position, episode)\n",
        "        \n",
        "        if done:\n",
        "            if state_1[0] >= 0.5:\n",
        "                # On successful epsisodes, adjust the following parameters\n",
        "\n",
        "                # Adjust epsilon\n",
        "                epsilon *= .99\n",
        "                writer.add_scalar('data/epsilon', epsilon, episode)\n",
        "\n",
        "                # Adjust learning rate\n",
        "                scheduler.step()\n",
        "                writer.add_scalar('data/learning_rate', optimizer.param_groups[0]['lr'], episode)\n",
        "\n",
        "                # Record successful episode\n",
        "                successes += 1\n",
        "                writer.add_scalar('data/cumulative_success', successes, episode)\n",
        "                writer.add_scalar('data/success', 1, episode)\n",
        "            \n",
        "            elif state_1[0] < 0.5:\n",
        "                writer.add_scalar('data/success', 0, episode)\n",
        "            \n",
        "            # Record history\n",
        "            loss_history.append(episode_loss)\n",
        "            reward_history.append(episode_reward)\n",
        "            writer.add_scalar('data/episode_loss', episode_loss, episode)\n",
        "            writer.add_scalar('data/episode_reward', episode_reward, episode)\n",
        "            weights = np.sum(np.abs(policy.l2.weight.data.numpy()))+np.sum(np.abs(policy.l1.weight.data.numpy()))\n",
        "            writer.add_scalar('data/weights', weights, episode)\n",
        "            writer.add_scalar('data/position', state_1[0], episode)\n",
        "            position.append(state_1[0])\n",
        "\n",
        "            break\n",
        "        else:\n",
        "            state = state_1\n",
        "            \n",
        "writer.close()\n",
        "print('successful episodes: {:d} - {:.4f}%'.format(successes, successes/episodes*100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/3000 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-8b91275cd914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mstate_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/success'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# Record history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'add_scalar'"
          ]
        }
      ]
    }
  ]
}